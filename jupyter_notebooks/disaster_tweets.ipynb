{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f98cc72",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "3ea7ab3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob  # imported to correct text\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# using version 4.20.1\n",
    "from transformers import DebertaTokenizer\n",
    "from transformers import DebertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239c537",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d21e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get zipped file name\n",
    "file = \"nlp-getting-started.zip\"\n",
    "\n",
    "# check if file for data exists and create if does not\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# unzip file and save to 'data' folder\n",
    "with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654ea3c",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets with disaster\n",
    "train_disaster_tweets = train[train.target==1].text\n",
    "\n",
    "# get tweets without disaster\n",
    "train_no_disaster_tweets = train[train.target==0].text\n",
    "\n",
    "# check if url is associated with disaster\n",
    "print(f\"Proportion of tweets associated with disaster with url: {100 * sum('http://' in t for t in train_disaster_tweets)/len(train_disaster_tweets)}%\")\n",
    "print(f\"Proportion of tweets not associated with disaster with url: {100 * sum('http://' in t for t in train_no_disaster_tweets)/len(train_no_disaster_tweets)}%\\n\")\n",
    "\n",
    "# check if mentions is associated with disaster\n",
    "print(f\"Proportion of tweets associated with disaster with @: {100 * sum('@' in t for t in train_disaster_tweets)/len(train_disaster_tweets)}%\")\n",
    "print(f\"Proportion of tweets not associated with disaster with @: {100 * sum('@' in t for t in train_no_disaster_tweets)/len(train_no_disaster_tweets)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfbc56",
   "metadata": {},
   "source": [
    "## Preprocess and encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "42634fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "def preprocess_text(doc):\n",
    "    \n",
    "    preprocessed_doc = []\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "    for text in doc:\n",
    "        \n",
    "        # make lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove urls\n",
    "        # text = tf.strings.regex_replace(text, \"(?:https?)?:\\/\\/t.co\\/\\w*\", \" \")\n",
    "        text = re.sub(r\"(?:https?)?:\\/\\/t.co\\/\\w*\", \" \", text)\n",
    "        \n",
    "        # remove mentions\n",
    "        # text = tf.strings.regex_replace(text, \"@\\w+\", \" \")\n",
    "        text = re.sub(r\"@\\w+\", \" \", text)\n",
    "        \n",
    "        # remove emoji pattern\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        \n",
    "        # correct typos\n",
    "        # text = TextBlob(text).correct().string\n",
    "        \n",
    "        # remove contractions\n",
    "        word_list = [contractions.fix(word) for word in text.split()]\n",
    "        \n",
    "        # remove non-alphabetical characters\n",
    "        word_list = [word for word in word_list if word.isalnum()]\n",
    "        \n",
    "        # remove stop words\n",
    "        word_list = [word for word in word_list if word not in stopwords]\n",
    "        \n",
    "        # join to single string\n",
    "        cleaned_tweet = \" \".join(word_list)\n",
    "        \n",
    "        # reappend to preprocessed doc\n",
    "        preprocessed_doc.append(cleaned_tweet)\n",
    "    \n",
    "    return preprocessed_doc\n",
    "\n",
    "\n",
    "def encode_text(texts, tokenizer, max_len=512):\n",
    "    \n",
    "    # list of ids and attention masks\n",
    "    encoded_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "                \n",
    "        # use tokenizer to encode text\n",
    "        encoded_text = tokenizer(\n",
    "            text = text,\n",
    "            max_length = max_len,\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        # extend id list\n",
    "        encoded_list.append(encoded_text[\"input_ids\"] + encoded_text[\"attention_mask\"])\n",
    "        \n",
    "    return torch.tensor(encoded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e950cc8",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "3c534d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "class Deberta_layer(nn.Module):  # inherits from torch.nn.Module\n",
    "    def __init__(self):\n",
    "        super(Deberta_layer, self).__init__()\n",
    "        self.deberta = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",     # base model\n",
    "            num_labels = 2,               # number of outputs\n",
    "            output_attentions = False,    # returns attention weights of all layers\n",
    "            output_hidden_states = False  # returns hidden states of all layers\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_masks: torch.Tensor,\n",
    "        target: Union[torch.FloatTensor, None]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        # if there is a target then return loss and prediction\n",
    "        if target != None:\n",
    "            output = self.deberta(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=attention_masks,\n",
    "                labels=target,\n",
    "                return_dict=None\n",
    "            )\n",
    "            \n",
    "            return output[\"loss\"], output[\"logits\"]\n",
    "        \n",
    "        else:\n",
    "            output = self.deberta(\n",
    "                intput_ids=input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=attention_masks,\n",
    "                labels=None,\n",
    "                return_dict=None\n",
    "            )\n",
    "            \n",
    "            return output[\"logits\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d83d7a",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "9d64c403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "lr = 3e-4\n",
    "betas = (0.9,0.98)\n",
    "eps = 1e-8\n",
    "n_splits = 5\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "max_len = 512\n",
    "\n",
    "# train model on training strata\n",
    "def train_model_strata(model, optimizer, input_ids, attention_masks):\n",
    "    \n",
    "    return\n",
    "\n",
    "# test model on testing strata\n",
    "def test_model_strata(model, optimizer, input_ids, attention_masks):\n",
    "    \n",
    "    return \n",
    "    \n",
    "# make a trained model\n",
    "def make_trained_model(lr, test_size, n_splits, n_epochs, batch_size, max_len, betas, eps, random_state):\n",
    "    \n",
    "    # read in data\n",
    "    train_data = pd.read_csv(\"./data/train.csv\")\n",
    "    test_data = pd.read_csv(\"./data/test.csv\")\n",
    "    \n",
    "    # split training data into text and targets\n",
    "    text = train_data.text.values.tolist()\n",
    "    y = torch.tensor(train_data.target)\n",
    "    \n",
    "    # shrink data\n",
    "    text = text[:100]\n",
    "    y = y[:100]\n",
    "    \n",
    "    # preprocess text\n",
    "#     preprocessed_text = preprocess_text(text)\n",
    "    preprocessed_text = text\n",
    "    \n",
    "    # get deberta tokenizer\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    \n",
    "    # encode text\n",
    "    X = encode_text(preprocessed_text, tokenizer, max_len)\n",
    "    \n",
    "    # check if cuda is available else use cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # create an instance of deberta model\n",
    "    model = Deberta_layer().to(device)\n",
    "\n",
    "    # add adamW optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=betas,\n",
    "        eps=eps\n",
    "    )\n",
    "    \n",
    "    # get stratified splitter\n",
    "    stratified_split = StratifiedShuffleSplit(\n",
    "        n_splits=n_splits,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # set best averaged validation loss and f1 score\n",
    "    best_averaged_val_accuracy = 0\n",
    "    best_averaged_val_loss = 1e6\n",
    "    best_averaged_val_f1_score = 0\n",
    "    \n",
    "    # iterate over strata\n",
    "    for strata, (train_index, test_index) in enumerate(stratified_split.split(X, y)):\n",
    "        \n",
    "        # split data\n",
    "        X_train = X[train_index,:]\n",
    "        X_test = X[test_index,:]\n",
    "        \n",
    "        # split targets\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "        # reshape targets to make dataloader\n",
    "        y_train_reshaped = y_train.reshape((y_train.shape[0],1))\n",
    "        y_test_reshaped = y_test.reshape((y_test.shape[0], 1))\n",
    "        \n",
    "        # make training dataloader\n",
    "        train_dataloader = DataLoader(\n",
    "            list(zip(X_train, y_train_reshaped)),\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True,\n",
    "#             sampler=,\n",
    "#             batch_sampler = ,\n",
    "            num_workers = 0\n",
    "        )\n",
    "        \n",
    "        # get linear learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = len(train_dataloader) * n_epochs\n",
    "        )\n",
    "        \n",
    "        # indicate training\n",
    "        model.train()\n",
    "                \n",
    "        # train model over strata\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            # time each epoch\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # track loss and f1\n",
    "            total_train_loss = 0\n",
    "            total_f1_score = 0\n",
    "            \n",
    "            # batch number\n",
    "            batch = 0\n",
    "            \n",
    "            for X_batch, y_batch in train_dataloader:\n",
    "                \n",
    "                # print batch number\n",
    "                print(f\"Batch {batch+1}\")\n",
    "                batch += 1\n",
    "                \n",
    "                # zero gradient\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # reshape data and targets for model\n",
    "                tuple_ids = X_batch[:,:max_len]\n",
    "                attention_masks = X_batch[:,max_len:]\n",
    "                labels = y_batch.flatten().to(float)\n",
    "                \n",
    "                # add to device\n",
    "                tuple_ids = tuple_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # get loss value and prediction\n",
    "                loss, logits = model(tuple_ids, attention_masks, labels)\n",
    "                \n",
    "                # # print\n",
    "                # print(f\"length: {len(train_dataloader)}\")\n",
    "                # print(tuple_ids)\n",
    "                # print(attention_masks)\n",
    "                # print(labels)\n",
    "                # print(logits)\n",
    "                \n",
    "                # add train loss\n",
    "                total_train_loss += loss\n",
    "                \n",
    "                # get prediction\n",
    "                y_batch_pred = (logits.flatten() < 0).to(float)\n",
    "                \n",
    "                # detach computational graph, copy to cpu, make numpy array\n",
    "                y_batch_pred = y_batch_pred.detach().cpu().numpy()\n",
    "                labels = labels.detach().cpu().numpy()                \n",
    "                \n",
    "                # calculate weighted f1 score of prediction\n",
    "                total_f1_score += f1_score(labels, y_batch_pred, average=\"weighted\")\n",
    "                \n",
    "                # accumulate gradient\n",
    "                loss.backward()\n",
    "                \n",
    "                # update parameters\n",
    "                optimizer.step()\n",
    "                \n",
    "                # update learning rate\n",
    "                scheduler.step()\n",
    "            \n",
    "            # gather data\n",
    "            average_train_loss = total_train_loss / len(train_dataloader)\n",
    "            average_f1_score = total_f1_score / len(train_dataloader)\n",
    "             \n",
    "            # print results\n",
    "            print(f\"Epoch: {epoch + 1}/{n_epochs}\")   \n",
    "            print(f\"Averaged train loss: {average_train_loss}\")\n",
    "            print(f\"Averaged f1 score: {average_f1_score}\")\n",
    "            print(f\"Training time: {datetime.timedelta(seconds = time.time()-t0)}\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # indicate testing\n",
    "            model.eval()\n",
    "            \n",
    "            # make testing dataloader\n",
    "            test_dataloader = DataLoader(\n",
    "                list(zip(X_test, y_test_reshaped)),\n",
    "                batch_size = batch_size,\n",
    "                shuffle = False,\n",
    "    #             sampler=,\n",
    "    #             batch_sampler =,\n",
    "                num_workers = 0\n",
    "            )\n",
    "            \n",
    "            # track validation loss and f1\n",
    "            total_val_train_loss = 0\n",
    "            total_val_f1_score = 0\n",
    "            \n",
    "            # batch number\n",
    "            batch = 0\n",
    "            \n",
    "            # disable gradient computation and reduce memory consumption\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for _, (X_val, y_val) in enumerate(test_dataloader):\n",
    "                                        \n",
    "                    # print batch number\n",
    "                    print(f\"Batch {batch + 1}\")\n",
    "                    batch += 1\n",
    "                    \n",
    "                    # print length\n",
    "                    print(len(test_dataloader))\n",
    "                    \n",
    "                    # reshape data and targets for model\n",
    "                    val_tuple_ids = X_val[:,:max_len]\n",
    "                    val_attention_masks = X_val[:,max_len:]\n",
    "                    val_labels = y_val.flatten().to(float)\n",
    "\n",
    "                    # add to device\n",
    "                    val_tuple_ids = val_tuple_ids.to(device)\n",
    "                    val_attention_masks = val_attention_masks.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    # get loss value and prediction\n",
    "                    val_loss, val_logits = model(val_tuple_ids, val_attention_masks, val_labels)\n",
    "                    \n",
    "                    # add train loss\n",
    "                    total_val_train_loss += val_loss\n",
    "\n",
    "                    # get prediction\n",
    "                    y_val_batch_pred = (val_logits.flatten() < 0).to(float)\n",
    "                \n",
    "                \n",
    "                    # detach computational graph, copy to cpu, make numpy array\n",
    "                    y_val_batch_pred = y_val_batch_pred.detach().cpu().numpy()\n",
    "                    val_labels = val_labels.detach().cpu().numpy()                \n",
    "\n",
    "                    # calculate weighted f1 score of prediction\n",
    "                    total_val_f1_score += f1_score(val_labels, y_val_batch_pred, average=\"weighted\")\n",
    "                    \n",
    "                # gather validation data\n",
    "                average_val_train_loss = total_val_train_loss / len(test_dataloader)\n",
    "                average_val_f1_score = total_val_f1_score / len(test_dataloader)\n",
    "\n",
    "                # print results\n",
    "                print(f\"Averaged validation loss: {average_val_train_loss}\")\n",
    "                print(f\"Averaged validation f1 score: {average_val_f1_score}\")\n",
    "                \n",
    "            \n",
    "            \n",
    "                # track best performance and save the model's state\n",
    "                if average_val_f1_score > best_averaged_val_f1_score:\n",
    "\n",
    "                    # update best scores\n",
    "                    best_averaged_val_loss = average_val_train_loss\n",
    "                    best_averaged_val_f1_score = average_val_f1_score\n",
    "\n",
    "                    # check if file for data exists and create if does not\n",
    "                    os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "                    # save path\n",
    "                    model_path = os.path.join(\"model\", \"model.pth\")\n",
    "\n",
    "                    # save model\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "07da69ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Epoch: 1/10\n",
      "Averaged train loss: 0.7806434039366976\n",
      "Averaged f1 score: 0.26499247129681913\n",
      "Training time: 0:15:45.164669\n",
      "\n",
      "Batch 1\n",
      "2\n",
      "Batch 2\n",
      "2\n",
      "Averaged validation loss: 0.3874912503958187\n",
      "Averaged validation f1 score: 0.2525\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Epoch: 2/10\n",
      "Averaged train loss: 0.271328942024449\n",
      "Averaged f1 score: 0.3248181818181818\n",
      "Training time: 0:08:54.063920\n",
      "\n",
      "Batch 1\n",
      "2\n",
      "Batch 2\n",
      "2\n",
      "Averaged validation loss: 0.24234015106559903\n",
      "Averaged validation f1 score: 0.2525\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmake_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[210], line 140\u001b[0m, in \u001b[0;36mmake_trained_model\u001b[1;34m(lr, test_size, n_splits, n_epochs, batch_size, max_len, betas, eps, random_state)\u001b[0m\n\u001b[0;32m    137\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# get loss value and prediction\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuple_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# # print\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# print(f\"length: {len(train_dataloader)}\")\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# print(tuple_ids)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# add train loss\u001b[39;00m\n\u001b[0;32m    150\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mDeberta_layer.forward\u001b[1;34m(self, input_ids, attention_masks, target)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     14\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# if there is a target then return loss and prediction\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1195\u001b[0m, in \u001b[0;36mDebertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1206\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1207\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:967\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    957\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    959\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    960\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    961\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    964\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    965\u001b[0m )\n\u001b[1;32m--> 967\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:463\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    453\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    454\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    455\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m         output_attentions,\n\u001b[0;32m    461\u001b[0m     )\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    473\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:376\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    369\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m ):\n\u001b[1;32m--> 376\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    385\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:309\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    302\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    318\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:663\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtalking_head:\n\u001b[0;32m    661\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_logits_proj(attention_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 663\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mXSoftmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtalking_head:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:114\u001b[0m, in \u001b[0;36mXSoftmax.forward\u001b[1;34m(self, input, mask, dim)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m dim\n\u001b[0;32m    112\u001b[0m rmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool))\n\u001b[1;32m--> 114\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[0;32m    116\u001b[0m output\u001b[38;5;241m.\u001b[39mmasked_fill_(rmask, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "make_trained_model(lr, test_size, n_splits, n_epochs, batch_size, max_len, betas, eps, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd498ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6e4fb843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(8.1070e+25, grad_fn=<MseLossBackward0>),\n",
       " tensor([ 0.2575, -0.0540,  0.0064, -0.2324, -0.1263, -0.0420, -0.0859,  0.0361,\n",
       "         -0.2909, -0.0551, -0.0938, -0.1085, -0.2714, -0.0300, -0.0789, -0.0170,\n",
       "         -0.2067, -0.0134, -0.1068, -0.0769,  0.2575], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "# train = pd.read_csv(\"./data/train.csv\")\n",
    "# etext = encode_text(train.text, tokenizer, 50)\n",
    "# a = np.reshape(np.array(train.target), (train.target.size,-1))\n",
    "# b = np.hstack((etext, a))\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = DebertaForSequenceClassification.from_pretrained(\n",
    "#             \"microsoft/deberta-base\",     # base model\n",
    "#             num_labels = 1,               # number of outputs\n",
    "#             output_attentions = False,    # returns attention weights of all layers\n",
    "#             output_hidden_states = False  # returns hidden states of all layers\n",
    "#         )\n",
    "# model.to(device)\n",
    "t = tokenizer(train.text[0])\n",
    "input_ids = torch.tensor(np.array(t[\"input_ids\"]))\n",
    "att_mask = torch.tensor(np.array(t[\"attention_mask\"]))\n",
    "labels=torch.FloatTensor(np.array(train.target[0]))\n",
    "\n",
    "input_ids = input_ids.reshape(input_ids.shape[0],1)\n",
    "att_mask = att_mask.reshape(att_mask.shape[0],1)\n",
    "labels = labels.reshape(labels.shape,1)\n",
    "# model(input_ids, token_type_ids=None, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Deberta_layer().to(device)\n",
    "model(input_ids, att_mask, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4a9be-ee39-44a6-ac0f-7f59f5a419b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "42c3139e-9e35-40d6-ac61-6b91014cfb50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  2522,   926, 12080,    32,     5, 31613,     9,    42,   849,\n",
      "         25581,  2253,  5113,   392, 12389, 15334,   286, 26650,   201,    70,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], dtype=torch.int32)\n",
      "tensor([1.], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.0551, dtype=torch.float64, grad_fn=<MseLossBackward0>), tensor([-0.0272], dtype=torch.float64, grad_fn=<ToCopyBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  2522,   926, 12080,    32,     5, 31613,     9,    42,   849,\n",
      "         25581,  2253,  5113,   392, 12389, 15334,   286, 26650,   201,    70,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], dtype=torch.int32)\n",
      "tensor([1.], dtype=torch.float64)\n",
      "SequenceClassifierOutput(loss=tensor(0.9894, dtype=torch.float64, grad_fn=<MseLossBackward0>), logits=tensor([0.0053], dtype=torch.float64, grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "\n",
      "tensor([[    1,  2522,   926,  ...,     9,    42,     2],\n",
      "        [    1, 42542,   668,  ..., 15531,     4,     2],\n",
      "        [    1,  3684,  1196,  ...,  4393,  1334,     2],\n",
      "        ...,\n",
      "        [    1,   448,   134,  ...,    35,  3387,     2],\n",
      "        [    1,  9497,  3219,  ..., 20974, 15268,     2],\n",
      "        [    1,   133,  9385,  ..., 16314,    30,     2]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "SequenceClassifierOutput(loss=tensor(0.4023, grad_fn=<MseLossBackward0>), logits=tensor([0.0026, 0.0044, 0.0043, 0.0059, 0.0059, 0.0063, 0.0059, 0.0060, 0.0042,\n",
      "        0.0035, 0.0035, 0.0041, 0.0053, 0.0042, 0.0045, 0.0059, 0.0077, 0.0038,\n",
      "        0.0060, 0.0041, 0.0048, 0.0055, 0.0049, 0.0046, 0.0058, 0.0057, 0.0060,\n",
      "        0.0043, 0.0051, 0.0043, 0.0037, 0.0078], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.2790, grad_fn=<MseLossBackward0>), logits=tensor([0.0040, 0.0059, 0.0063, 0.0015, 0.0028, 0.0043, 0.0057, 0.0046, 0.0029,\n",
      "        0.0037, 0.0027, 0.0041, 0.0039, 0.0038, 0.0041, 0.0060, 0.0042, 0.0068,\n",
      "        0.0042, 0.0054, 0.0079, 0.0023, 0.0040, 0.0035, 0.0047, 0.0056, 0.0041,\n",
      "        0.0053, 0.0047, 0.0038, 0.0061, 0.0054], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4030, grad_fn=<MseLossBackward0>), logits=tensor([0.0067, 0.0059, 0.0020, 0.0065, 0.0049, 0.0065, 0.0053, 0.0029, 0.0040,\n",
      "        0.0063, 0.0029, 0.0038, 0.0037, 0.0046, 0.0044, 0.0023, 0.0034, 0.0024,\n",
      "        0.0057, 0.0030, 0.0043, 0.0046, 0.0056, 0.0057, 0.0041, 0.0045, 0.0039,\n",
      "        0.0049, 0.0050, 0.0042, 0.0026, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4026, grad_fn=<MseLossBackward0>), logits=tensor([0.0069, 0.0040, 0.0056, 0.0028, 0.0042, 0.0063, 0.0043, 0.0059, 0.0067,\n",
      "        0.0009, 0.0042, 0.0020, 0.0045, 0.0048, 0.0056, 0.0058, 0.0044, 0.0068,\n",
      "        0.0049, 0.0059, 0.0088, 0.0088, 0.0038, 0.0052, 0.0055, 0.0044, 0.0054,\n",
      "        0.0071, 0.0031, 0.0058, 0.0051, 0.0049], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4954, grad_fn=<MseLossBackward0>), logits=tensor([0.0045, 0.0051, 0.0042, 0.0040, 0.0069, 0.0074, 0.0020, 0.0067, 0.0051,\n",
      "        0.0053, 0.0049, 0.0053, 0.0030, 0.0068, 0.0082, 0.0046, 0.0056, 0.0039,\n",
      "        0.0070, 0.0046, 0.0035, 0.0032, 0.0050, 0.0065, 0.0032, 0.0032, 0.0057,\n",
      "        0.0063, 0.0049, 0.0022, 0.0068, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4649, grad_fn=<MseLossBackward0>), logits=tensor([0.0035, 0.0066, 0.0060, 0.0037, 0.0059, 0.0050, 0.0049, 0.0053, 0.0056,\n",
      "        0.0065, 0.0016, 0.0036, 0.0036, 0.0044, 0.0038, 0.0039, 0.0053, 0.0047,\n",
      "        0.0026, 0.0062, 0.0033, 0.0048, 0.0034, 0.0034, 0.0050, 0.0062, 0.0022,\n",
      "        0.0052, 0.0041, 0.0061, 0.0059, 0.0040], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4333, grad_fn=<MseLossBackward0>), logits=tensor([0.0044, 0.0040, 0.0027, 0.0037, 0.0050, 0.0058, 0.0053, 0.0044, 0.0058,\n",
      "        0.0058, 0.0035, 0.0044, 0.0049, 0.0051, 0.0088, 0.0056, 0.0025, 0.0047,\n",
      "        0.0036, 0.0026, 0.0048, 0.0060, 0.0068, 0.0068, 0.0056, 0.0047, 0.0039,\n",
      "        0.0049, 0.0055, 0.0045, 0.0052, 0.0031], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4643, grad_fn=<MseLossBackward0>), logits=tensor([0.0058, 0.0055, 0.0075, 0.0053, 0.0046, 0.0058, 0.0062, 0.0035, 0.0058,\n",
      "        0.0036, 0.0039, 0.0039, 0.0058, 0.0060, 0.0043, 0.0038, 0.0040, 0.0046,\n",
      "        0.0049, 0.0058, 0.0043, 0.0043, 0.0050, 0.0052, 0.0035, 0.0087, 0.0037,\n",
      "        0.0047, 0.0032, 0.0048, 0.0042, 0.0027], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4343, grad_fn=<MseLossBackward0>), logits=tensor([0.0055, 0.0037, 0.0061, 0.0014, 0.0032, 0.0048, 0.0048, 0.0052, 0.0037,\n",
      "        0.0049, 0.0061, 0.0038, 0.0027, 0.0042, 0.0032, 0.0045, 0.0017, 0.0055,\n",
      "        0.0031, 0.0028, 0.0066, 0.0040, 0.0020, 0.0049, 0.0051, 0.0053, 0.0047,\n",
      "        0.0037, 0.0049, 0.0072, 0.0017, 0.0055], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4957, grad_fn=<MseLossBackward0>), logits=tensor([0.0031, 0.0055, 0.0048, 0.0034, 0.0056, 0.0049, 0.0067, 0.0034, 0.0051,\n",
      "        0.0063, 0.0066, 0.0073, 0.0033, 0.0050, 0.0035, 0.0047, 0.0023, 0.0057,\n",
      "        0.0047, 0.0066, 0.0047, 0.0049, 0.0049, 0.0036, 0.0016, 0.0055, 0.0045,\n",
      "        0.0046, 0.0066, 0.0070, 0.0054, 0.0048], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3720, grad_fn=<MseLossBackward0>), logits=tensor([0.0025, 0.0042, 0.0019, 0.0026, 0.0036, 0.0032, 0.0036, 0.0046, 0.0078,\n",
      "        0.0057, 0.0044, 0.0079, 0.0058, 0.0038, 0.0063, 0.0063, 0.0041, 0.0042,\n",
      "        0.0021, 0.0040, 0.0061, 0.0051, 0.0048, 0.0041, 0.0035, 0.0076, 0.0040,\n",
      "        0.0047, 0.0066, 0.0012, 0.0020, 0.0072], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5269, grad_fn=<MseLossBackward0>), logits=tensor([0.0043, 0.0049, 0.0023, 0.0036, 0.0035, 0.0038, 0.0033, 0.0037, 0.0065,\n",
      "        0.0060, 0.0048, 0.0045, 0.0041, 0.0047, 0.0061, 0.0026, 0.0027, 0.0032,\n",
      "        0.0021, 0.0070, 0.0040, 0.0045, 0.0054, 0.0040, 0.0035, 0.0035, 0.0043,\n",
      "        0.0029, 0.0043, 0.0080, 0.0051, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3407, grad_fn=<MseLossBackward0>), logits=tensor([0.0042, 0.0055, 0.0030, 0.0035, 0.0057, 0.0041, 0.0058, 0.0066, 0.0043,\n",
      "        0.0044, 0.0066, 0.0055, 0.0037, 0.0073, 0.0056, 0.0028, 0.0040, 0.0033,\n",
      "        0.0025, 0.0059, 0.0053, 0.0047, 0.0052, 0.0038, 0.0042, 0.0066, 0.0064,\n",
      "        0.0051, 0.0048, 0.0068, 0.0047, 0.0025], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5575, grad_fn=<MseLossBackward0>), logits=tensor([0.0057, 0.0047, 0.0047, 0.0059, 0.0060, 0.0029, 0.0039, 0.0026, 0.0073,\n",
      "        0.0022, 0.0043, 0.0026, 0.0045, 0.0052, 0.0061, 0.0039, 0.0035, 0.0048,\n",
      "        0.0075, 0.0069, 0.0043, 0.0060, 0.0037, 0.0058, 0.0050, 0.0038, 0.0049,\n",
      "        0.0042, 0.0031, 0.0046, 0.0053, 0.0052], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3409, grad_fn=<MseLossBackward0>), logits=tensor([0.0048, 0.0045, 0.0020, 0.0045, 0.0050, 0.0038, 0.0026, 0.0032, 0.0052,\n",
      "        0.0047, 0.0022, 0.0054, 0.0053, 0.0035, 0.0048, 0.0049, 0.0045, 0.0046,\n",
      "        0.0053, 0.0055, 0.0052, 0.0041, 0.0059, 0.0039, 0.0037, 0.0028, 0.0023,\n",
      "        0.0064, 0.0024, 0.0049, 0.0052, 0.0041], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4650, grad_fn=<MseLossBackward0>), logits=tensor([0.0040, 0.0048, 0.0044, 0.0033, 0.0036, 0.0057, 0.0059, 0.0048, 0.0064,\n",
      "        0.0045, 0.0043, 0.0053, 0.0048, 0.0046, 0.0009, 0.0041, 0.0051, 0.0032,\n",
      "        0.0037, 0.0025, 0.0039, 0.0052, 0.0031, 0.0025, 0.0052, 0.0043, 0.0021,\n",
      "        0.0027, 0.0037, 0.0048, 0.0066, 0.0041], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3714, grad_fn=<MseLossBackward0>), logits=tensor([0.0033, 0.0053, 0.0049, 0.0062, 0.0071, 0.0033, 0.0024, 0.0054, 0.0037,\n",
      "        0.0042, 0.0053, 0.0060, 0.0044, 0.0048, 0.0059, 0.0051, 0.0051, 0.0061,\n",
      "        0.0050, 0.0048, 0.0043, 0.0049, 0.0031, 0.0072, 0.0060, 0.0051, 0.0038,\n",
      "        0.0044, 0.0057, 0.0042, 0.0047, 0.0039], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3717, grad_fn=<MseLossBackward0>), logits=tensor([0.0057, 0.0041, 0.0044, 0.0042, 0.0039, 0.0058, 0.0044, 0.0065, 0.0048,\n",
      "        0.0060, 0.0060, 0.0050, 0.0042, 0.0032, 0.0049, 0.0041, 0.0044, 0.0044,\n",
      "        0.0068, 0.0040, 0.0060, 0.0033, 0.0059, 0.0055, 0.0045, 0.0055, 0.0041,\n",
      "        0.0043, 0.0052, 0.0060, 0.0047, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3720, grad_fn=<MseLossBackward0>), logits=tensor([0.0044, 0.0052, 0.0057, 0.0045, 0.0056, 0.0041, 0.0043, 0.0054, 0.0024,\n",
      "        0.0056, 0.0066, 0.0009, 0.0027, 0.0103, 0.0036, 0.0029, 0.0031, 0.0052,\n",
      "        0.0056, 0.0053, 0.0055, 0.0051, 0.0053, 0.0049, 0.0029, 0.0060, 0.0037,\n",
      "        0.0065, 0.0046, 0.0061, 0.0064, 0.0058], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4332, grad_fn=<MseLossBackward0>), logits=tensor([0.0064, 0.0048, 0.0052, 0.0056, 0.0054, 0.0034, 0.0051, 0.0071, 0.0045,\n",
      "        0.0043, 0.0039, 0.0053, 0.0052, 0.0060, 0.0052, 0.0068, 0.0028, 0.0045,\n",
      "        0.0060, 0.0036, 0.0050, 0.0043, 0.0020, 0.0059, 0.0031, 0.0053, 0.0088,\n",
      "        0.0019, 0.0046, 0.0062, 0.0057, 0.0046], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5263, grad_fn=<MseLossBackward0>), logits=tensor([0.0037, 0.0078, 0.0068, 0.0038, 0.0033, 0.0078, 0.0051, 0.0053, 0.0056,\n",
      "        0.0046, 0.0022, 0.0054, 0.0044, 0.0054, 0.0061, 0.0037, 0.0057, 0.0031,\n",
      "        0.0044, 0.0045, 0.0032, 0.0046, 0.0064, 0.0062, 0.0047, 0.0046, 0.0060,\n",
      "        0.0034, 0.0058, 0.0056, 0.0048, 0.0033], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3098, grad_fn=<MseLossBackward0>), logits=tensor([0.0033, 0.0047, 0.0047, 0.0039, 0.0037, 0.0037, 0.0010, 0.0056, 0.0058,\n",
      "        0.0064, 0.0031, 0.0052, 0.0014, 0.0045, 0.0053, 0.0055, 0.0042, 0.0039,\n",
      "        0.0047, 0.0047, 0.0057, 0.0041, 0.0046, 0.0038, 0.0046, 0.0060, 0.0045,\n",
      "        0.0051, 0.0029, 0.0032, 0.0047, 0.0062], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[548], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m other_label \u001b[38;5;241m=\u001b[39m other_label\u001b[38;5;241m.\u001b[39mreshape((other_label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     96\u001b[0m other_label \u001b[38;5;241m=\u001b[39m other_label\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnew_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_label\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1181\u001b[0m, in \u001b[0;36mDebertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1193\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:954\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    944\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    946\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    947\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    948\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    951\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    952\u001b[0m )\n\u001b[1;32m--> 954\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    961\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:447\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    438\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    439\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    440\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m         rel_embeddings,\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    457\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:352\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    351\u001b[0m ):\n\u001b[1;32m--> 352\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    361\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:285\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    278\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ):\n\u001b[1;32m--> 285\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    294\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:609\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03mCall the module\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m     qp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# .split(self.all_head_size, dim=-1)\u001b[39;00m\n\u001b[0;32m    610\u001b[0m     query_layer, key_layer, value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(qp)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "t = encode_text(train.text[:1], tokenizer, 50)\n",
    "labels = train.target[:1]\n",
    "\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(t[:,:50])\n",
    "att_mask = torch.tensor(t[:,50:])\n",
    "labels = torch.tensor(labels).to(float).reshape(labels.shape, 1)\n",
    "print(input_ids)\n",
    "print(att_mask)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = input_ids.reshape(input_ids.shape[0], 1)\n",
    "# att_mask = att_mask.reshape(att_mask.shape[0], 1)\n",
    "labels = labels.reshape(labels.shape[0], 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Deberta_layer().to(device)\n",
    "print(model(input_ids, att_mask, labels))\n",
    "\n",
    "\n",
    "# input_ids = []\n",
    "# att_masks = []\n",
    "# for text in train.text[:2]:\n",
    "    \n",
    "\n",
    "new_model = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",     # base model\n",
    "            num_labels = 1,               # number of outputs\n",
    "            output_attentions = False,    # returns attention weights of all layers\n",
    "            output_hidden_states = False  # returns hidden states of all layers\n",
    "        )\n",
    "\n",
    "new_input_ids = torch.tensor(t[0,:50]).reshape(1,50)\n",
    "new_masks = torch.tensor(t[0,50:]).reshape(1,50)\n",
    "new_label = train.target[:1]\n",
    "new_label = torch.tensor(new_label).to(float).reshape(new_label.shape, 1)\n",
    "\n",
    "print(new_input_ids)\n",
    "print(new_masks)\n",
    "print(new_label)\n",
    "\n",
    "\n",
    "print(new_model(new_input_ids, token_type_ids=None, attention_mask=new_masks, labels=new_label))\n",
    "print(\"\\n\")\n",
    "\n",
    "texts = train.text.values.tolist()\n",
    "\n",
    "other_input_ids = []\n",
    "other_masks = []\n",
    "for text in texts:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text=text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=10,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    other_input_ids.append(encoded_text.get(\"input_ids\"))\n",
    "    other_masks.append(encoded_text.get(\"attention_mask\"))\n",
    "\n",
    "other_input_ids = torch.cat(other_input_ids, dim=0)\n",
    "other_masks = torch.cat(other_masks, dim=0)\n",
    "\n",
    "other_label = torch.tensor(train.target.values.tolist())\n",
    "\n",
    "print(other_input_ids)\n",
    "print(other_masks)\n",
    "print(other_label)\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    other_input_ids,\n",
    "    other_masks,\n",
    "    other_label\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    other_input_ids, other_masks, other_label = batch\n",
    "    other_label = other_label.reshape((other_label.shape[0], 1))\n",
    "    other_label = other_label.float()\n",
    "    print(new_model(other_input_ids, token_type_ids=None, attention_mask=other_masks, labels=other_label))\n",
    "\n",
    "\n",
    "\n",
    "# other_label = torch.tensor(train.target[:1])\n",
    "# other_label = other_label.reshape((1, 1))\n",
    "# other_label = other_label.float()\n",
    "# print(other_input_ids)\n",
    "# print(other_masks)\n",
    "# print(other_label)\n",
    "# new_model(other_input_ids, token_type_ids=None, attention_mask=other_masks, labels=other_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "eb30b87a-5dee-4d90-801a-2491eee02b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "(a <= 2).to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "6a4fdefc-b8c4-45f1-b98c-44d3ba994ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\owner\\anaconda3\\lib\\site-packages (4.20.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\~-kenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a072a0-1b9b-4578-8204-f89eac2a02d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Batch 1 / 25\n",
      "Batch 2 / 25\n",
      "Batch 3 / 25\n",
      "Batch 4 / 25\n",
      "Batch 5 / 25\n",
      "Batch 6 / 25\n",
      "Batch 7 / 25\n",
      "Batch 8 / 25\n",
      "Batch 9 / 25\n",
      "Batch 10 / 25\n",
      "Batch 11 / 25\n",
      "Batch 12 / 25\n",
      "Batch 13 / 25\n",
      "Batch 14 / 25\n",
      "Batch 15 / 25\n",
      "Batch 16 / 25\n",
      "Batch 17 / 25\n",
      "Batch 18 / 25\n",
      "Batch 19 / 25\n",
      "Batch 20 / 25\n",
      "Batch 21 / 25\n",
      "Batch 22 / 25\n",
      "Batch 23 / 25\n",
      "Batch 24 / 25\n",
      "Batch 25 / 25\n",
      "Averaged train accuracy: 0.69625\n",
      "Averaged train loss: 0.5624345743656158\n",
      "Averaged f1 score: 0.6153133735001934\n",
      "Training time: 0:07:54.187415\n",
      "\n",
      "Batch 1 / 2\n",
      "Batch 2 / 2\n",
      "Validation accuracy: 0.6138392857142857\n",
      "Averaged validation loss: 0.8914835155010223\n",
      "Averaged validation f1 score: 0.5677480082942268\n",
      "\n",
      "best_average_train_accuracy: 0.69625\n",
      "best_average_train_loss: 0.5624345743656158\n",
      "best_average_train_f1_score: 0.6153133735001934\n",
      "best_average_val_accuracy: 0.6138392857142857\n",
      "best_average_val_loss: 0.8914835155010223\n",
      "best_average_val_f1_score: 0.5677480082942268\n",
      "\n",
      "\n",
      "Epoch: 2/10\n",
      "Batch 1 / 25\n",
      "Batch 2 / 25\n",
      "Batch 3 / 25\n",
      "Batch 4 / 25\n",
      "Batch 5 / 25\n",
      "Batch 6 / 25\n",
      "Batch 7 / 25\n",
      "Batch 8 / 25\n",
      "Batch 9 / 25\n",
      "Batch 10 / 25\n",
      "Batch 11 / 25\n",
      "Batch 12 / 25\n",
      "Batch 13 / 25\n",
      "Batch 14 / 25\n",
      "Batch 15 / 25\n",
      "Batch 16 / 25\n",
      "Batch 17 / 25\n",
      "Batch 18 / 25\n",
      "Batch 19 / 25\n",
      "Batch 20 / 25\n",
      "Batch 21 / 25\n",
      "Batch 22 / 25\n",
      "Batch 23 / 25\n",
      "Batch 24 / 25\n",
      "Batch 25 / 25\n",
      "Averaged train accuracy: 0.8025\n",
      "Averaged train loss: 0.44585274636745453\n",
      "Averaged f1 score: 0.7947145634699672\n",
      "Training time: 0:07:57.840895\n",
      "\n",
      "Batch 1 / 2\n",
      "Batch 2 / 2\n",
      "Validation accuracy: 0.6004464285714286\n",
      "Averaged validation loss: 0.9507246017456055\n",
      "Averaged validation f1 score: 0.5907421498231701\n",
      "\n",
      "best_average_train_accuracy: 0.8025\n",
      "best_average_train_loss: 0.44585274636745453\n",
      "best_average_train_f1_score: 0.7947145634699672\n",
      "best_average_val_accuracy: 0.6004464285714286\n",
      "best_average_val_loss: 0.9507246017456055\n",
      "best_average_val_f1_score: 0.5907421498231701\n",
      "\n",
      "\n",
      "Epoch: 3/10\n",
      "Batch 1 / 25\n",
      "Batch 2 / 25\n",
      "Batch 3 / 25\n",
      "Batch 4 / 25\n",
      "Batch 5 / 25\n",
      "Batch 6 / 25\n",
      "Batch 7 / 25\n",
      "Batch 8 / 25\n",
      "Batch 9 / 25\n",
      "Batch 10 / 25\n",
      "Batch 11 / 25\n",
      "Batch 12 / 25\n",
      "Batch 13 / 25\n",
      "Batch 14 / 25\n",
      "Batch 15 / 25\n",
      "Batch 16 / 25\n",
      "Batch 17 / 25\n",
      "Batch 18 / 25\n",
      "Batch 19 / 25\n",
      "Batch 20 / 25\n",
      "Batch 21 / 25\n",
      "Batch 22 / 25\n",
      "Batch 23 / 25\n",
      "Batch 24 / 25\n",
      "Batch 25 / 25\n",
      "Averaged train accuracy: 0.85375\n",
      "Averaged train loss: 0.3584923541545868\n",
      "Averaged f1 score: 0.8493406913457406\n",
      "Training time: 0:08:04.569611\n",
      "\n",
      "Batch 1 / 2\n",
      "Batch 2 / 2\n",
      "Validation accuracy: 0.5379464285714286\n",
      "Averaged validation loss: 1.1375692486763\n",
      "Averaged validation f1 score: 0.5396026132233029\n",
      "\n",
      "best_average_train_accuracy: 0.8025\n",
      "best_average_train_loss: 0.44585274636745453\n",
      "best_average_train_f1_score: 0.7947145634699672\n",
      "best_average_val_accuracy: 0.6004464285714286\n",
      "best_average_val_loss: 0.9507246017456055\n",
      "best_average_val_f1_score: 0.5907421498231701\n",
      "\n",
      "\n",
      "Epoch: 4/10\n",
      "Batch 1 / 25\n",
      "Batch 2 / 25\n",
      "Batch 3 / 25\n",
      "Batch 4 / 25\n",
      "Batch 5 / 25\n",
      "Batch 6 / 25\n",
      "Batch 7 / 25\n",
      "Batch 8 / 25\n",
      "Batch 9 / 25\n",
      "Batch 10 / 25\n",
      "Batch 11 / 25\n",
      "Batch 12 / 25\n",
      "Batch 13 / 25\n",
      "Batch 14 / 25\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "class params():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        lr = 2e-5,\n",
    "        test_size = 0.2,\n",
    "        n_splits = 5,\n",
    "        n_epochs = 100,\n",
    "        batch_size = 32,\n",
    "        max_len = 100,\n",
    "        betas = (0.9, 0.98),\n",
    "        eps = 1e-8,\n",
    "        random_state = 42\n",
    "    ) -> None:\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.test_size = test_size\n",
    "        self.n_splits = n_splits\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "# store best statistics\n",
    "class best_stats(make_model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        best_averaged_train_accuracy = 0,\n",
    "        best_averaged_train_loss = 1e6,\n",
    "        best_averaged_train_f1_score = 0,\n",
    "        best_averaged_val_accuracy = 0,\n",
    "        best_averaged_val_loss = 1e6,\n",
    "        best_averaged_val_f1_score = 0\n",
    "    ) -> None:\n",
    "        self.best_average_train_accuracy = best_averaged_train_accuracy\n",
    "        self.best_average_train_loss = best_averaged_train_loss\n",
    "        self.best_average_train_f1_score = best_averaged_train_f1_score\n",
    "        self.best_average_val_accuracy = best_averaged_val_accuracy\n",
    "        self.best_average_val_loss = best_averaged_val_loss\n",
    "        self.best_average_val_f1_score = best_averaged_val_f1_score\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        # get attributes\n",
    "        attrs = vars(self)\n",
    "        \n",
    "        return \"\\n\".join(\"%s: %s\" % item for item in attrs.items())\n",
    "        \n",
    "    def record_best(\n",
    "        self,\n",
    "        model: make_model,\n",
    "        average_train_accuracy: float,\n",
    "        average_train_loss: float,\n",
    "        average_train_f1_score: float,\n",
    "        average_val_accuracy: float,\n",
    "        average_val_loss: float,\n",
    "        average_val_f1_score: float\n",
    "    ) -> None:\n",
    "        \n",
    "        # track best performance and save the model's state\n",
    "        if average_val_f1_score > self.best_average_val_f1_score:\n",
    "\n",
    "            # update best model scores\n",
    "            self.best_average_train_accuracy = average_train_accuracy\n",
    "            self.best_average_train_loss = average_train_loss\n",
    "            self.best_average_train_f1_score = average_train_f1_score\n",
    "            self.best_average_val_accuracy = average_val_accuracy\n",
    "            self.best_average_val_loss = average_val_loss\n",
    "            self.best_average_val_f1_score = average_val_f1_score\n",
    "\n",
    "            # check if file for data exists and create if does not\n",
    "            os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "            # save path\n",
    "            model_path = os.path.join(\"model\", \"model.pth\")\n",
    "\n",
    "            # save model\n",
    "            torch.save(model.model.state_dict(), model_path)\n",
    "       \n",
    "    \n",
    "# create model\n",
    "class make_model(params):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Deberta_layer().to(device)\n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        optimizer: torch.optim.AdamW,\n",
    "        scheduler: get_linear_schedule_with_warmup\n",
    "    ) -> Tuple[float, float, float]:\n",
    "        \n",
    "        # time each training epoch\n",
    "        t0 = time.time()\n",
    "\n",
    "        # track accuracy, loss, and f1\n",
    "        total_train_accuracy = 0\n",
    "        total_train_loss = 0\n",
    "        total_train_f1_score = 0\n",
    "\n",
    "        # batch number\n",
    "        batch = 0\n",
    "        \n",
    "        # indicate training\n",
    "        self.model.train()\n",
    "        \n",
    "        for X_batch, y_batch in dataloader:\n",
    "            \n",
    "            # print batch number\n",
    "            print(f\"Batch {batch+1} / {len(dataloader)}\")\n",
    "            batch += 1\n",
    "\n",
    "            # zero gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # reshape data and targets for model\n",
    "            tuple_ids = X_batch[:,:params.max_len]\n",
    "            attention_masks = X_batch[:,params.max_len:]\n",
    "            labels = y_batch.flatten().to(float)\n",
    "\n",
    "            # add to device\n",
    "            tuple_ids = tuple_ids.to(params.device)\n",
    "            attention_masks = attention_masks.to(params.device)\n",
    "            labels = labels.to(params.device)\n",
    "\n",
    "            # get loss value and prediction\n",
    "            loss, logits = self.model(tuple_ids, attention_masks, labels)\n",
    "\n",
    "            # add train loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # get prediction\n",
    "            y_batch_pred = (torch.argmax(logits, axis=1).flatten() > 0.5).to(float)\n",
    "\n",
    "            # detach computational graph, copy to cpu, make numpy array\n",
    "            y_batch_pred = y_batch_pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            \n",
    "            # compute training accuracy\n",
    "            total_train_accuracy += np.sum(y_batch_pred == labels) / len(labels)\n",
    "\n",
    "            # calculate weighted f1 score of prediction\n",
    "            total_train_f1_score += f1_score(labels, y_batch_pred, average=\"weighted\")\n",
    "\n",
    "            # accumulate gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "        # gather data\n",
    "        average_train_accuracy = total_train_accuracy / len(dataloader)\n",
    "        average_train_loss = total_train_loss / len(dataloader)\n",
    "        average_train_f1_score = total_train_f1_score / len(dataloader)\n",
    "\n",
    "        # print results\n",
    "        print(f\"Averaged train accuracy: {average_train_accuracy}\")\n",
    "        print(f\"Averaged train loss: {average_train_loss}\")\n",
    "        print(f\"Averaged f1 score: {average_train_f1_score}\")\n",
    "        print(f\"Training time: {datetime.timedelta(seconds = time.time()-t0)}\\n\")\n",
    "\n",
    "        return average_train_accuracy, average_train_loss, average_train_f1_score\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, dataloader: torch.utils.data.DataLoader) -> Tuple[float, float, float]:\n",
    "\n",
    "        # batch number\n",
    "        batch = 0\n",
    "        \n",
    "        # track validation accuracy, validation loss, and f1\n",
    "        total_val_accuracy = 0\n",
    "        total_val_loss = 0\n",
    "        total_val_f1_score = 0\n",
    "        \n",
    "        # indicate testing\n",
    "        self.model.eval()\n",
    "        \n",
    "        # disable gradient computation and reduce memory consumption\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            for X_val, y_val in dataloader:\n",
    "\n",
    "                # print batch number\n",
    "                print(f\"Batch {batch + 1} / {len(dataloader)}\")\n",
    "                batch += 1\n",
    "\n",
    "                # reshape data and targets for model\n",
    "                val_tuple_ids = X_val[:,:params.max_len]\n",
    "                val_attention_masks = X_val[:,params.max_len:]\n",
    "                val_labels = y_val.flatten().to(float)\n",
    "                \n",
    "                # add to device\n",
    "                val_tuple_ids = val_tuple_ids.to(params.device)\n",
    "                val_attention_masks = val_attention_masks.to(params.device)\n",
    "                val_labels = val_labels.to(params.device)\n",
    "\n",
    "                # get loss value and prediction\n",
    "                val_loss, val_logits = self.model(val_tuple_ids, val_attention_masks, val_labels)\n",
    "\n",
    "                # add train loss\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # get prediction\n",
    "                y_val_batch_pred = (torch.argmax(val_logits, axis=1).flatten() > 0.5).to(float)\n",
    "\n",
    "                # detach computational graph, copy to cpu, make numpy array\n",
    "                y_val_batch_pred = y_val_batch_pred.detach().cpu().numpy()\n",
    "                val_labels = val_labels.detach().cpu().numpy()\n",
    "                \n",
    "                # calculate accuracy\n",
    "                total_val_accuracy += np.sum(y_val_batch_pred == val_labels) / len(val_labels)\n",
    "\n",
    "                # calculate weighted f1 score of prediction\n",
    "                total_val_f1_score += f1_score(val_labels, y_val_batch_pred, average=\"weighted\")\n",
    "                \n",
    "        # gather validation data\n",
    "        average_val_accuracy = total_val_accuracy / len(dataloader)\n",
    "        average_val_loss = total_val_loss / len(dataloader)\n",
    "        average_val_f1_score = total_val_f1_score / len(dataloader)\n",
    "        \n",
    "        # print results\n",
    "        print(f\"Validation accuracy: {average_val_accuracy}\")\n",
    "        print(f\"Averaged validation loss: {average_val_loss}\")\n",
    "        print(f\"Averaged validation f1 score: {average_val_f1_score}\\n\")\n",
    "        \n",
    "        return average_val_accuracy, average_val_loss, average_val_f1_score\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "# check if cuda is available else use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set parameters\n",
    "params = params(device = device)\n",
    "        \n",
    "# read in data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# split training data into text and targets\n",
    "text = train_data.text.values.tolist()\n",
    "y = torch.tensor(train_data.target)\n",
    "\n",
    "# shrink data\n",
    "text = text[:1000]\n",
    "y = y[:1000]\n",
    "\n",
    "# preprocess text\n",
    "preprocessed_text = preprocess_text(text)\n",
    "\n",
    "# get deberta tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# encode text\n",
    "X = encode_text(preprocessed_text, tokenizer, params.max_len)\n",
    "\n",
    "# create an instance of deberta model\n",
    "test_model = make_model()\n",
    "\n",
    "# add adamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=test_model.model.parameters(),\n",
    "    lr=params.lr,\n",
    "    betas=params.betas,\n",
    "    eps=params.eps\n",
    ")\n",
    "\n",
    "# get stratified splitter\n",
    "# stratified_split = StratifiedShuffleSplit(\n",
    "#     n_splits=params.n_splits,\n",
    "#     test_size=params.test_size,\n",
    "#     random_state=params.random_state\n",
    "# )\n",
    "\n",
    "# record best model stats\n",
    "model_best_stats = best_stats()\n",
    "\n",
    "# iterate over strata\n",
    "# for strata, (train_index, test_index) in enumerate(stratified_split.split(X, y)):\n",
    "    \n",
    "    # # print strata\n",
    "    # print(f\"Strata: {strata + 1}\\n\")\n",
    "\n",
    "#     # split data\n",
    "#     X_train = X[train_index,:]\n",
    "#     X_test = X[test_index,:]\n",
    "\n",
    "#     # split targets\n",
    "#     y_train = y[train_index]\n",
    "#     y_test = y[test_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create dataset\n",
    "dataset = list(zip(X, y.reshape(y.shape[0],1)))\n",
    "\n",
    "# set sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# split data\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# make training dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size = params.batch_size\n",
    ")\n",
    "\n",
    "# make testing dataloader\n",
    "test_dataloder = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler = SequentialSampler(test_dataset),\n",
    "    batch_size = params.batch_size\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # reshape targets to make dataloader\n",
    "    # y_train_reshaped = y_train.reshape((y_train.shape[0],1))\n",
    "    # y_test_reshaped = y_test.reshape((y_test.shape[0], 1))\n",
    "\n",
    "\n",
    "#     # make training dataloader\n",
    "#     train_dataloader = DataLoader(\n",
    "#         list(zip(X_train, y_train_reshaped)),\n",
    "#         batch_size = params.batch_size,\n",
    "#         shuffle = True,\n",
    "#         num_workers = 0\n",
    "#     )\n",
    "\n",
    "#     # make testing dataloader\n",
    "#     test_dataloader = DataLoader(\n",
    "#         list(zip(X, y_test_reshaped)),\n",
    "#         batch_size = params.batch_size,\n",
    "#         shuffle = False,\n",
    "#         num_workers = 0\n",
    "#     )\n",
    "\n",
    "# get linear learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = len(train_dataloader) * params.n_epochs\n",
    ")\n",
    "    \n",
    "    # train model over strata\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # print epoch\n",
    "    print(f\"Epoch: {epoch + 1}/{n_epochs}\")\n",
    "\n",
    "    # train model\n",
    "    average_train_accuracy, average_train_loss, average_train_f1_score = test_model.train(train_dataloader, optimizer, scheduler)\n",
    "\n",
    "    # test model\n",
    "    average_val_accuracy, average_val_loss, average_val_f1_score = test_model.test(test_dataloader)\n",
    "\n",
    "    # record best model\n",
    "    model_best_stats.record_best(\n",
    "        test_model,\n",
    "        average_train_accuracy,\n",
    "        average_train_loss,\n",
    "        average_train_f1_score,\n",
    "        average_val_accuracy,\n",
    "        average_val_loss,\n",
    "        average_val_f1_score\n",
    "    )\n",
    "\n",
    "    print(model_best_stats)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # read in data\n",
    "# train_data = pd.read_csv(\"./data/train.csv\")\n",
    "# test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# # split training data into text and targets\n",
    "# text = train_data.text.values.tolist()\n",
    "# y = torch.tensor(train_data.target)\n",
    "\n",
    "# # shrink data\n",
    "# text = text[:100]\n",
    "# y = y[:100]\n",
    "\n",
    "# # preprocess text\n",
    "# preprocessed_text = text\n",
    "\n",
    "# # get deberta tokenizer\n",
    "# tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# # encode text\n",
    "# X = encode_text(preprocessed_text, tokenizer, max_len)\n",
    "\n",
    "# # reshape targets to make dataloader\n",
    "# y_test_reshaped = y.reshape((y.shape[0], 1))\n",
    "\n",
    "# # indicate testing\n",
    "# test_model.eval()\n",
    "            \n",
    "# # make testing dataloader\n",
    "# test_dataloader = DataLoader(\n",
    "# list(zip(X, y_test_reshaped)),\n",
    "# batch_size = batch_size,\n",
    "# shuffle = False,\n",
    "# num_workers = 0\n",
    "# )\n",
    "        \n",
    "# average_val_accuracy, average_val_train_loss, average_val_f1_score = test_model.test(test_dataloader)\n",
    "\n",
    "# # print results\n",
    "# print(f\"Validation accuracy: {average_val_accuracy}\")\n",
    "# print(f\"Averaged validation loss: {average_val_train_loss}\")\n",
    "# print(f\"Averaged validation f1 score: {average_val_f1_score}\")\n",
    "\n",
    "\n",
    "# # set best averaged validation loss and f1 score\n",
    "# best_averaged_val_accuracy = 0\n",
    "# best_averaged_val_loss = 1e6\n",
    "# best_averaged_val_f1_score = 0\n",
    "\n",
    "# test_model.record_best(\n",
    "#     best_averaged_val_accuracy,\n",
    "#     best_averaged_val_loss,\n",
    "#     best_averaged_val_f1_score,\n",
    "#     average_val_accuracy,\n",
    "#     average_val_train_loss,\n",
    "#     average_val_f1_score\n",
    "# )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445a1f6-46c4-4608-b806-db605d1abf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f59cd4-3e4c-4715-a9d7-9082269f8859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311eeb0-613c-41ee-8ac7-148735e17682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545c0ef-52e2-487f-afa0-55d2aa9cab94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd56d7-f103-4548-8c6c-b4ae625d4017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d114b-1301-4f15-a726-3bb52685831a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e920b-7482-4578-aaed-8d4f1489d329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e354f88-529c-4e0a-93ef-dfd92ceac51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
