{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f98cc72",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea7ab3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from textblob import TextBlob  # imported to correct text\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# using version 4.20.1\n",
    "from transformers import DebertaTokenizer\n",
    "from transformers import DebertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239c537",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d21e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get zipped file name\n",
    "file = \"nlp-getting-started.zip\"\n",
    "\n",
    "# check if file for data exists and create if does not\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# unzip file and save to 'data' folder\n",
    "with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654ea3c",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets with disaster\n",
    "train_disaster_tweets = train[train.target==1].text\n",
    "\n",
    "# get tweets without disaster\n",
    "train_no_disaster_tweets = train[train.target==0].text\n",
    "\n",
    "# check if url is associated with disaster\n",
    "print(f\"Proportion of tweets associated with disaster with url: {100 * sum('http://' in t for t in train_disaster_tweets)/len(train_disaster_tweets)}%\")\n",
    "print(f\"Proportion of tweets not associated with disaster with url: {100 * sum('http://' in t for t in train_no_disaster_tweets)/len(train_no_disaster_tweets)}%\\n\")\n",
    "\n",
    "# check if mentions is associated with disaster\n",
    "print(f\"Proportion of tweets associated with disaster with @: {100 * sum('@' in t for t in train_disaster_tweets)/len(train_disaster_tweets)}%\")\n",
    "print(f\"Proportion of tweets not associated with disaster with @: {100 * sum('@' in t for t in train_no_disaster_tweets)/len(train_no_disaster_tweets)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfbc56",
   "metadata": {},
   "source": [
    "## Preprocess and encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42634fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "def preprocess_text(doc):\n",
    "    \n",
    "    preprocessed_doc = []\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "    for text in doc:\n",
    "        \n",
    "        # make lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove mentions\n",
    "        text = tf.strings.regex_replace(text, \"@\\w+\", \" \")\n",
    "        \n",
    "        # correct typos\n",
    "        text = TextBlob(text.numpy().decode(\"utf-8\")).correct().string\n",
    "        \n",
    "        # tokenize by word\n",
    "        word_tokens = word_tokenize(text)\n",
    "        \n",
    "        # remove non-alphabetical characters\n",
    "        word_tokens = [word for word in word_tokens if word.isalnum()]\n",
    "        \n",
    "        # remove stop words\n",
    "        word_tokens = [word for word in word_tokens if word not in stopwords]\n",
    "        \n",
    "        # apply stemmer\n",
    "        stemmer = PorterStemmer()\n",
    "        word_tokens = [stemmer.stem(word) for word in word_tokens]\n",
    "        \n",
    "        # reappend to preprocessed doc\n",
    "        preprocessed_doc.append(word_tokens)\n",
    "    \n",
    "    return preprocessed_doc\n",
    "\n",
    "\n",
    "def encode_text(texts, tokenizer, max_len=512):\n",
    "    \n",
    "    # list of ids and attention masks\n",
    "    encoded_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "                \n",
    "        # use tokenizer to encode text\n",
    "        encoded_text = tokenizer(\n",
    "            text = text,\n",
    "            max_length = max_len,\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        # extend id list\n",
    "        encoded_list.append(encoded_text[\"input_ids\"] + encoded_text[\"attention_mask\"])\n",
    "        \n",
    "    return torch.tensor(encoded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e950cc8",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c534d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "class Deberta_layer(nn.Module):  # inherits from torch.nn.Module\n",
    "    def __init__(self):\n",
    "        super(Deberta_layer, self).__init__()\n",
    "        self.deberta = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",     # base model\n",
    "            num_labels = 1,               # number of outputs\n",
    "            output_attentions = False,    # returns attention weights of all layers\n",
    "            output_hidden_states = False  # returns hidden states of all layers\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_masks: torch.Tensor,\n",
    "        target: Union[torch.FloatTensor, None]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        # if there is a target then return loss and prediction\n",
    "        if target != None:\n",
    "            output = self.deberta(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=attention_masks,\n",
    "                labels=target,\n",
    "                return_dict=None\n",
    "            )\n",
    "            \n",
    "            return output[\"loss\"], output[\"logits\"]\n",
    "        \n",
    "        else:\n",
    "            output = self.deberta(\n",
    "                intput_ids=input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=attention_masks,\n",
    "                labels=None,\n",
    "                return_dict=None\n",
    "            )\n",
    "            \n",
    "            return output[\"logits\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b3f7a",
   "metadata": {},
   "source": [
    "## Create Classes for Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fafe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d83d7a",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9d64c403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "lr = 3e-4\n",
    "betas = (0.9,0.98)\n",
    "eps = 1e-8\n",
    "n_splits = 5\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "max_len = 512\n",
    "\n",
    "# train model on training strata\n",
    "def train_model_strata(model, optimizer, input_ids, attention_masks):\n",
    "    \n",
    "    return\n",
    "\n",
    "# test model on testing strata\n",
    "def test_model_strata(model, optimizer, input_ids, attention_masks):\n",
    "    \n",
    "    return \n",
    "    \n",
    "# make a trained model\n",
    "def make_trained_model(lr, test_size, n_splits, n_epochs, batch_size, max_len, betas, eps, random_state):\n",
    "    \n",
    "    # read in data\n",
    "    train_data = pd.read_csv(\"./data/train.csv\")\n",
    "    test_data = pd.read_csv(\"./data/test.csv\")\n",
    "    \n",
    "    # split training data into text and targets\n",
    "    text = train_data.text.values.tolist()\n",
    "    y = torch.tensor(train_data.target)\n",
    "    \n",
    "    # preprocess text\n",
    "#     preprocessed_text = preprocess_text(text)\n",
    "    preprocessed_text = text\n",
    "    \n",
    "    # get deberta tokenizer\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    \n",
    "    # encode text\n",
    "    X = encode_text(preprocessed_text, tokenizer, max_len)\n",
    "    \n",
    "    # check if cuda is available else use cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # create an instance of deberta model\n",
    "    model = Deberta_layer().to(device)\n",
    "\n",
    "    # add adamW optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=betas,\n",
    "        eps=eps\n",
    "    )\n",
    "    \n",
    "    # get stratified splitter\n",
    "    stratified_split = StratifiedShuffleSplit(\n",
    "        n_splits=n_splits,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # set best averaged validation loss and f1 score\n",
    "    best_averaged_val_loss = 1e6\n",
    "    best_averaged_val_f1_score = 0\n",
    "    \n",
    "    # iterate over strata\n",
    "    for strata, (train_index, test_index) in enumerate(stratified_split.split(X, y)):\n",
    "        \n",
    "        # split data\n",
    "        X_train = X[train_index,:]\n",
    "        X_test = X[test_index,:]\n",
    "        \n",
    "        # split targets\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "        # reshape targets to make dataloader\n",
    "        y_train_reshaped = y_train.reshape((y_train.shape[0],1))\n",
    "        y_test_reshaped = y_test.reshape((y_test.shape[0], 1))\n",
    "        \n",
    "        # make training dataloader\n",
    "        train_dataloader = DataLoader(\n",
    "            list(zip(X_train, y_train_reshaped)),\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True,\n",
    "#             sampler=,\n",
    "#             batch_sampler = ,\n",
    "            num_workers = 0\n",
    "        )\n",
    "        \n",
    "        # get linear learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = len(train_dataloader) * n_epochs\n",
    "        )\n",
    "        \n",
    "        # indicate training\n",
    "        model.train()\n",
    "                \n",
    "        # train model over strata\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            # time each epoch\n",
    "            t0 = time.time()\n",
    "            print(f\"Epoch: {(epoch+1)}/{n_epochs}\")\n",
    "            \n",
    "            # track loss and f1\n",
    "            total_train_loss = 0\n",
    "            total_f1_score = 0\n",
    "            \n",
    "            # batch number\n",
    "            batch = 0\n",
    "            \n",
    "            for X_batch, y_batch in train_dataloader:\n",
    "                \n",
    "                # # print batch number\n",
    "                # print(f\"Batch {batch+1}\")\n",
    "                batch += 1\n",
    "                \n",
    "#                 # zero gradient\n",
    "#                 optimizer.zero_grad()\n",
    "                \n",
    "#                 # reshape data and targets for model\n",
    "#                 tuple_ids = X_batch[:,:max_len]\n",
    "#                 attention_masks = X_batch[:,max_len:]\n",
    "#                 labels = y_batch.flatten().to(float)\n",
    "                \n",
    "#                 # add to device\n",
    "#                 tuple_ids = tuple_ids.to(device)\n",
    "#                 attention_masks = attention_masks.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "                \n",
    "#                 # get loss value and prediction\n",
    "#                 loss, logits = model(tuple_ids, attention_masks, labels)\n",
    "                \n",
    "                # # print\n",
    "                # print(f\"length: {len(train_dataloader)}\")\n",
    "                # print(tuple_ids)\n",
    "                # print(attention_masks)\n",
    "                # print(labels)\n",
    "                # print(logits)\n",
    "                \n",
    "#                 # add train loss\n",
    "#                 total_train_loss += loss\n",
    "                \n",
    "#                 # get prediction\n",
    "#                 y_batch_pred = (logits.flatten() < 0).to(float)\n",
    "                \n",
    "#                 # detach computational graph, copy to cpu, make numpy array\n",
    "#                 y_batch_pred = y_batch_pred.detach().cpu().numpy()\n",
    "#                 labels = labels.detach().cpu().numpy()                \n",
    "                \n",
    "#                 # calculate weighted f1 score of prediction\n",
    "#                 total_f1_score += f1_score(labels, y_batch_pred, average=\"weighted\")\n",
    "                \n",
    "#                 # accumulate gradient\n",
    "#                 loss.backward()\n",
    "                \n",
    "#                 # update parameters\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#                 # update learning rate\n",
    "#                 scheduler.step()\n",
    "            \n",
    "#             # gather data\n",
    "#             average_train_loss = total_train_loss / len(train_dataloader)\n",
    "#             average_f1_score = total_f1_score / len(train_dataloader)\n",
    "             \n",
    "#             # print results\n",
    "#             print(f\"Epoch: {epoch}/{n_epochs}\")   \n",
    "#             print(f\"Averaged train loss: {average_train_loss}\")\n",
    "#             print(f\"Averaged f1 score: {average_f1_score}\")\n",
    "#             print(f\"Training time: {format_time(time.time()-t0)}\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # indicate testing\n",
    "            model.eval()\n",
    "            \n",
    "            # make testing dataloader\n",
    "            test_dataloader = DataLoader(\n",
    "                list(zip(X_test, y_test_reshaped)),\n",
    "                batch_size = batch_size,\n",
    "                shuffle = False,\n",
    "    #             sampler=,\n",
    "    #             batch_sampler =,\n",
    "                num_workers = 0\n",
    "            )\n",
    "            \n",
    "            # track validation loss and f1\n",
    "            total_val_train_loss = 0\n",
    "            total_val_f1_score = 0\n",
    "            \n",
    "            # batch number\n",
    "            batch = 0\n",
    "            \n",
    "            # disable gradient computation and reduce memory consumption\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for _, (X_val, y_val) in enumerate(test_dataloader):\n",
    "                    \n",
    "                    # print batch number\n",
    "                    print(f\"Batch {batch + 1}\")\n",
    "                    batch += 1\n",
    "                    \n",
    "                    # print length\n",
    "                    print(len(test_dataloader))\n",
    "                    \n",
    "                    # reshape data and targets for model\n",
    "                    val_tuple_ids = X_val[:,:max_len]\n",
    "                    val_attention_masks = X_val[:,max_len:]\n",
    "                    val_labels = y_val.flatten().to(float)\n",
    "\n",
    "                    # add to device\n",
    "                    val_tuple_ids = val_tuple_ids.to(device)\n",
    "                    val_attention_masks = val_attention_masks.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    # get loss value and prediction\n",
    "                    val_loss, val_logits = model(val_tuple_ids, val_attention_masks, val_labels)\n",
    "                    \n",
    "                    # add train loss\n",
    "                    total_val_train_loss += val_loss\n",
    "\n",
    "                    # get prediction\n",
    "                    y_val_batch_pred = (val_logits.flatten() < 0).to(float)\n",
    "                \n",
    "                \n",
    "                    # detach computational graph, copy to cpu, make numpy array\n",
    "                    y_val_batch_pred = y_val_batch_pred.detach().cpu().numpy()\n",
    "                    val_labels = val_labels.detach().cpu().numpy()                \n",
    "\n",
    "                    # calculate weighted f1 score of prediction\n",
    "                    total_val_f1_score += f1_score(val_labels, y_val_batch_pred, average=\"weighted\")\n",
    "                    \n",
    "                # gather validation data\n",
    "                average_val_train_loss = total_val_train_loss / len(test_dataloader)\n",
    "                average_val_f1_score = total_val_f1_score / len(test_dataloader)\n",
    "\n",
    "                # print results\n",
    "                print(f\"Averaged validation loss: {average_val_train_loss}\")\n",
    "                print(f\"Averaged validation f1 score: {average_val_f1_score}\")\n",
    "                \n",
    "            \n",
    "            \n",
    "            # track best performance and save the model's state\n",
    "            if averaged_val_f1_score > best_averaged_val_f1_score:\n",
    "                \n",
    "                # update best scores\n",
    "                best_averaged_val_loss = averaged_val_train_loss\n",
    "                best_averaged_val_f1_score = averaged_val_f1_score\n",
    "                \n",
    "                # check if file for data exists and create if does not\n",
    "                os.makedirs(\"model\", exist_ok=True)\n",
    "                \n",
    "                # save path\n",
    "                model_path = os.path.join(\"model\", \"model.pth\")\n",
    "                \n",
    "                # save model\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da69ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_trained_model(lr, test_size, n_splits, n_epochs, batch_size, max_len, betas, eps, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd498ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6e4fb843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(8.1070e+25, grad_fn=<MseLossBackward0>),\n",
       " tensor([ 0.2575, -0.0540,  0.0064, -0.2324, -0.1263, -0.0420, -0.0859,  0.0361,\n",
       "         -0.2909, -0.0551, -0.0938, -0.1085, -0.2714, -0.0300, -0.0789, -0.0170,\n",
       "         -0.2067, -0.0134, -0.1068, -0.0769,  0.2575], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "# train = pd.read_csv(\"./data/train.csv\")\n",
    "# etext = encode_text(train.text, tokenizer, 50)\n",
    "# a = np.reshape(np.array(train.target), (train.target.size,-1))\n",
    "# b = np.hstack((etext, a))\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = DebertaForSequenceClassification.from_pretrained(\n",
    "#             \"microsoft/deberta-base\",     # base model\n",
    "#             num_labels = 1,               # number of outputs\n",
    "#             output_attentions = False,    # returns attention weights of all layers\n",
    "#             output_hidden_states = False  # returns hidden states of all layers\n",
    "#         )\n",
    "# model.to(device)\n",
    "t = tokenizer(train.text[0])\n",
    "input_ids = torch.tensor(np.array(t[\"input_ids\"]))\n",
    "att_mask = torch.tensor(np.array(t[\"attention_mask\"]))\n",
    "labels=torch.FloatTensor(np.array(train.target[0]))\n",
    "\n",
    "input_ids = input_ids.reshape(input_ids.shape[0],1)\n",
    "att_mask = att_mask.reshape(att_mask.shape[0],1)\n",
    "labels = labels.reshape(labels.shape,1)\n",
    "# model(input_ids, token_type_ids=None, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Deberta_layer().to(device)\n",
    "model(input_ids, att_mask, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4a9be-ee39-44a6-ac0f-7f59f5a419b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "42c3139e-9e35-40d6-ac61-6b91014cfb50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  2522,   926, 12080,    32,     5, 31613,     9,    42,   849,\n",
      "         25581,  2253,  5113,   392, 12389, 15334,   286, 26650,   201,    70,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], dtype=torch.int32)\n",
      "tensor([1.], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.0551, dtype=torch.float64, grad_fn=<MseLossBackward0>), tensor([-0.0272], dtype=torch.float64, grad_fn=<ToCopyBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  2522,   926, 12080,    32,     5, 31613,     9,    42,   849,\n",
      "         25581,  2253,  5113,   392, 12389, 15334,   286, 26650,   201,    70,\n",
      "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], dtype=torch.int32)\n",
      "tensor([1.], dtype=torch.float64)\n",
      "SequenceClassifierOutput(loss=tensor(0.9894, dtype=torch.float64, grad_fn=<MseLossBackward0>), logits=tensor([0.0053], dtype=torch.float64, grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "\n",
      "tensor([[    1,  2522,   926,  ...,     9,    42,     2],\n",
      "        [    1, 42542,   668,  ..., 15531,     4,     2],\n",
      "        [    1,  3684,  1196,  ...,  4393,  1334,     2],\n",
      "        ...,\n",
      "        [    1,   448,   134,  ...,    35,  3387,     2],\n",
      "        [    1,  9497,  3219,  ..., 20974, 15268,     2],\n",
      "        [    1,   133,  9385,  ..., 16314,    30,     2]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "SequenceClassifierOutput(loss=tensor(0.4023, grad_fn=<MseLossBackward0>), logits=tensor([0.0026, 0.0044, 0.0043, 0.0059, 0.0059, 0.0063, 0.0059, 0.0060, 0.0042,\n",
      "        0.0035, 0.0035, 0.0041, 0.0053, 0.0042, 0.0045, 0.0059, 0.0077, 0.0038,\n",
      "        0.0060, 0.0041, 0.0048, 0.0055, 0.0049, 0.0046, 0.0058, 0.0057, 0.0060,\n",
      "        0.0043, 0.0051, 0.0043, 0.0037, 0.0078], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.2790, grad_fn=<MseLossBackward0>), logits=tensor([0.0040, 0.0059, 0.0063, 0.0015, 0.0028, 0.0043, 0.0057, 0.0046, 0.0029,\n",
      "        0.0037, 0.0027, 0.0041, 0.0039, 0.0038, 0.0041, 0.0060, 0.0042, 0.0068,\n",
      "        0.0042, 0.0054, 0.0079, 0.0023, 0.0040, 0.0035, 0.0047, 0.0056, 0.0041,\n",
      "        0.0053, 0.0047, 0.0038, 0.0061, 0.0054], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4030, grad_fn=<MseLossBackward0>), logits=tensor([0.0067, 0.0059, 0.0020, 0.0065, 0.0049, 0.0065, 0.0053, 0.0029, 0.0040,\n",
      "        0.0063, 0.0029, 0.0038, 0.0037, 0.0046, 0.0044, 0.0023, 0.0034, 0.0024,\n",
      "        0.0057, 0.0030, 0.0043, 0.0046, 0.0056, 0.0057, 0.0041, 0.0045, 0.0039,\n",
      "        0.0049, 0.0050, 0.0042, 0.0026, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4026, grad_fn=<MseLossBackward0>), logits=tensor([0.0069, 0.0040, 0.0056, 0.0028, 0.0042, 0.0063, 0.0043, 0.0059, 0.0067,\n",
      "        0.0009, 0.0042, 0.0020, 0.0045, 0.0048, 0.0056, 0.0058, 0.0044, 0.0068,\n",
      "        0.0049, 0.0059, 0.0088, 0.0088, 0.0038, 0.0052, 0.0055, 0.0044, 0.0054,\n",
      "        0.0071, 0.0031, 0.0058, 0.0051, 0.0049], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4954, grad_fn=<MseLossBackward0>), logits=tensor([0.0045, 0.0051, 0.0042, 0.0040, 0.0069, 0.0074, 0.0020, 0.0067, 0.0051,\n",
      "        0.0053, 0.0049, 0.0053, 0.0030, 0.0068, 0.0082, 0.0046, 0.0056, 0.0039,\n",
      "        0.0070, 0.0046, 0.0035, 0.0032, 0.0050, 0.0065, 0.0032, 0.0032, 0.0057,\n",
      "        0.0063, 0.0049, 0.0022, 0.0068, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4649, grad_fn=<MseLossBackward0>), logits=tensor([0.0035, 0.0066, 0.0060, 0.0037, 0.0059, 0.0050, 0.0049, 0.0053, 0.0056,\n",
      "        0.0065, 0.0016, 0.0036, 0.0036, 0.0044, 0.0038, 0.0039, 0.0053, 0.0047,\n",
      "        0.0026, 0.0062, 0.0033, 0.0048, 0.0034, 0.0034, 0.0050, 0.0062, 0.0022,\n",
      "        0.0052, 0.0041, 0.0061, 0.0059, 0.0040], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4333, grad_fn=<MseLossBackward0>), logits=tensor([0.0044, 0.0040, 0.0027, 0.0037, 0.0050, 0.0058, 0.0053, 0.0044, 0.0058,\n",
      "        0.0058, 0.0035, 0.0044, 0.0049, 0.0051, 0.0088, 0.0056, 0.0025, 0.0047,\n",
      "        0.0036, 0.0026, 0.0048, 0.0060, 0.0068, 0.0068, 0.0056, 0.0047, 0.0039,\n",
      "        0.0049, 0.0055, 0.0045, 0.0052, 0.0031], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4643, grad_fn=<MseLossBackward0>), logits=tensor([0.0058, 0.0055, 0.0075, 0.0053, 0.0046, 0.0058, 0.0062, 0.0035, 0.0058,\n",
      "        0.0036, 0.0039, 0.0039, 0.0058, 0.0060, 0.0043, 0.0038, 0.0040, 0.0046,\n",
      "        0.0049, 0.0058, 0.0043, 0.0043, 0.0050, 0.0052, 0.0035, 0.0087, 0.0037,\n",
      "        0.0047, 0.0032, 0.0048, 0.0042, 0.0027], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4343, grad_fn=<MseLossBackward0>), logits=tensor([0.0055, 0.0037, 0.0061, 0.0014, 0.0032, 0.0048, 0.0048, 0.0052, 0.0037,\n",
      "        0.0049, 0.0061, 0.0038, 0.0027, 0.0042, 0.0032, 0.0045, 0.0017, 0.0055,\n",
      "        0.0031, 0.0028, 0.0066, 0.0040, 0.0020, 0.0049, 0.0051, 0.0053, 0.0047,\n",
      "        0.0037, 0.0049, 0.0072, 0.0017, 0.0055], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4957, grad_fn=<MseLossBackward0>), logits=tensor([0.0031, 0.0055, 0.0048, 0.0034, 0.0056, 0.0049, 0.0067, 0.0034, 0.0051,\n",
      "        0.0063, 0.0066, 0.0073, 0.0033, 0.0050, 0.0035, 0.0047, 0.0023, 0.0057,\n",
      "        0.0047, 0.0066, 0.0047, 0.0049, 0.0049, 0.0036, 0.0016, 0.0055, 0.0045,\n",
      "        0.0046, 0.0066, 0.0070, 0.0054, 0.0048], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3720, grad_fn=<MseLossBackward0>), logits=tensor([0.0025, 0.0042, 0.0019, 0.0026, 0.0036, 0.0032, 0.0036, 0.0046, 0.0078,\n",
      "        0.0057, 0.0044, 0.0079, 0.0058, 0.0038, 0.0063, 0.0063, 0.0041, 0.0042,\n",
      "        0.0021, 0.0040, 0.0061, 0.0051, 0.0048, 0.0041, 0.0035, 0.0076, 0.0040,\n",
      "        0.0047, 0.0066, 0.0012, 0.0020, 0.0072], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5269, grad_fn=<MseLossBackward0>), logits=tensor([0.0043, 0.0049, 0.0023, 0.0036, 0.0035, 0.0038, 0.0033, 0.0037, 0.0065,\n",
      "        0.0060, 0.0048, 0.0045, 0.0041, 0.0047, 0.0061, 0.0026, 0.0027, 0.0032,\n",
      "        0.0021, 0.0070, 0.0040, 0.0045, 0.0054, 0.0040, 0.0035, 0.0035, 0.0043,\n",
      "        0.0029, 0.0043, 0.0080, 0.0051, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3407, grad_fn=<MseLossBackward0>), logits=tensor([0.0042, 0.0055, 0.0030, 0.0035, 0.0057, 0.0041, 0.0058, 0.0066, 0.0043,\n",
      "        0.0044, 0.0066, 0.0055, 0.0037, 0.0073, 0.0056, 0.0028, 0.0040, 0.0033,\n",
      "        0.0025, 0.0059, 0.0053, 0.0047, 0.0052, 0.0038, 0.0042, 0.0066, 0.0064,\n",
      "        0.0051, 0.0048, 0.0068, 0.0047, 0.0025], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5575, grad_fn=<MseLossBackward0>), logits=tensor([0.0057, 0.0047, 0.0047, 0.0059, 0.0060, 0.0029, 0.0039, 0.0026, 0.0073,\n",
      "        0.0022, 0.0043, 0.0026, 0.0045, 0.0052, 0.0061, 0.0039, 0.0035, 0.0048,\n",
      "        0.0075, 0.0069, 0.0043, 0.0060, 0.0037, 0.0058, 0.0050, 0.0038, 0.0049,\n",
      "        0.0042, 0.0031, 0.0046, 0.0053, 0.0052], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3409, grad_fn=<MseLossBackward0>), logits=tensor([0.0048, 0.0045, 0.0020, 0.0045, 0.0050, 0.0038, 0.0026, 0.0032, 0.0052,\n",
      "        0.0047, 0.0022, 0.0054, 0.0053, 0.0035, 0.0048, 0.0049, 0.0045, 0.0046,\n",
      "        0.0053, 0.0055, 0.0052, 0.0041, 0.0059, 0.0039, 0.0037, 0.0028, 0.0023,\n",
      "        0.0064, 0.0024, 0.0049, 0.0052, 0.0041], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4650, grad_fn=<MseLossBackward0>), logits=tensor([0.0040, 0.0048, 0.0044, 0.0033, 0.0036, 0.0057, 0.0059, 0.0048, 0.0064,\n",
      "        0.0045, 0.0043, 0.0053, 0.0048, 0.0046, 0.0009, 0.0041, 0.0051, 0.0032,\n",
      "        0.0037, 0.0025, 0.0039, 0.0052, 0.0031, 0.0025, 0.0052, 0.0043, 0.0021,\n",
      "        0.0027, 0.0037, 0.0048, 0.0066, 0.0041], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3714, grad_fn=<MseLossBackward0>), logits=tensor([0.0033, 0.0053, 0.0049, 0.0062, 0.0071, 0.0033, 0.0024, 0.0054, 0.0037,\n",
      "        0.0042, 0.0053, 0.0060, 0.0044, 0.0048, 0.0059, 0.0051, 0.0051, 0.0061,\n",
      "        0.0050, 0.0048, 0.0043, 0.0049, 0.0031, 0.0072, 0.0060, 0.0051, 0.0038,\n",
      "        0.0044, 0.0057, 0.0042, 0.0047, 0.0039], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3717, grad_fn=<MseLossBackward0>), logits=tensor([0.0057, 0.0041, 0.0044, 0.0042, 0.0039, 0.0058, 0.0044, 0.0065, 0.0048,\n",
      "        0.0060, 0.0060, 0.0050, 0.0042, 0.0032, 0.0049, 0.0041, 0.0044, 0.0044,\n",
      "        0.0068, 0.0040, 0.0060, 0.0033, 0.0059, 0.0055, 0.0045, 0.0055, 0.0041,\n",
      "        0.0043, 0.0052, 0.0060, 0.0047, 0.0053], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3720, grad_fn=<MseLossBackward0>), logits=tensor([0.0044, 0.0052, 0.0057, 0.0045, 0.0056, 0.0041, 0.0043, 0.0054, 0.0024,\n",
      "        0.0056, 0.0066, 0.0009, 0.0027, 0.0103, 0.0036, 0.0029, 0.0031, 0.0052,\n",
      "        0.0056, 0.0053, 0.0055, 0.0051, 0.0053, 0.0049, 0.0029, 0.0060, 0.0037,\n",
      "        0.0065, 0.0046, 0.0061, 0.0064, 0.0058], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4332, grad_fn=<MseLossBackward0>), logits=tensor([0.0064, 0.0048, 0.0052, 0.0056, 0.0054, 0.0034, 0.0051, 0.0071, 0.0045,\n",
      "        0.0043, 0.0039, 0.0053, 0.0052, 0.0060, 0.0052, 0.0068, 0.0028, 0.0045,\n",
      "        0.0060, 0.0036, 0.0050, 0.0043, 0.0020, 0.0059, 0.0031, 0.0053, 0.0088,\n",
      "        0.0019, 0.0046, 0.0062, 0.0057, 0.0046], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5263, grad_fn=<MseLossBackward0>), logits=tensor([0.0037, 0.0078, 0.0068, 0.0038, 0.0033, 0.0078, 0.0051, 0.0053, 0.0056,\n",
      "        0.0046, 0.0022, 0.0054, 0.0044, 0.0054, 0.0061, 0.0037, 0.0057, 0.0031,\n",
      "        0.0044, 0.0045, 0.0032, 0.0046, 0.0064, 0.0062, 0.0047, 0.0046, 0.0060,\n",
      "        0.0034, 0.0058, 0.0056, 0.0048, 0.0033], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3098, grad_fn=<MseLossBackward0>), logits=tensor([0.0033, 0.0047, 0.0047, 0.0039, 0.0037, 0.0037, 0.0010, 0.0056, 0.0058,\n",
      "        0.0064, 0.0031, 0.0052, 0.0014, 0.0045, 0.0053, 0.0055, 0.0042, 0.0039,\n",
      "        0.0047, 0.0047, 0.0057, 0.0041, 0.0046, 0.0038, 0.0046, 0.0060, 0.0045,\n",
      "        0.0051, 0.0029, 0.0032, 0.0047, 0.0062], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[548], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m other_label \u001b[38;5;241m=\u001b[39m other_label\u001b[38;5;241m.\u001b[39mreshape((other_label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     96\u001b[0m other_label \u001b[38;5;241m=\u001b[39m other_label\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnew_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_label\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1181\u001b[0m, in \u001b[0;36mDebertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1193\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:954\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    944\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    946\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    947\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    948\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    951\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    952\u001b[0m )\n\u001b[1;32m--> 954\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    961\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:447\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    438\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    439\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    440\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m         rel_embeddings,\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    457\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:352\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    351\u001b[0m ):\n\u001b[1;32m--> 352\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    361\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:285\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    278\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ):\n\u001b[1;32m--> 285\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    294\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:609\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03mCall the module\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m     qp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# .split(self.all_head_size, dim=-1)\u001b[39;00m\n\u001b[0;32m    610\u001b[0m     query_layer, key_layer, value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(qp)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "t = encode_text(train.text[:1], tokenizer, 50)\n",
    "labels = train.target[:1]\n",
    "\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(t[:,:50])\n",
    "att_mask = torch.tensor(t[:,50:])\n",
    "labels = torch.tensor(labels).to(float).reshape(labels.shape, 1)\n",
    "print(input_ids)\n",
    "print(att_mask)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = input_ids.reshape(input_ids.shape[0], 1)\n",
    "# att_mask = att_mask.reshape(att_mask.shape[0], 1)\n",
    "labels = labels.reshape(labels.shape[0], 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Deberta_layer().to(device)\n",
    "print(model(input_ids, att_mask, labels))\n",
    "\n",
    "\n",
    "# input_ids = []\n",
    "# att_masks = []\n",
    "# for text in train.text[:2]:\n",
    "    \n",
    "\n",
    "new_model = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",     # base model\n",
    "            num_labels = 1,               # number of outputs\n",
    "            output_attentions = False,    # returns attention weights of all layers\n",
    "            output_hidden_states = False  # returns hidden states of all layers\n",
    "        )\n",
    "\n",
    "new_input_ids = torch.tensor(t[0,:50]).reshape(1,50)\n",
    "new_masks = torch.tensor(t[0,50:]).reshape(1,50)\n",
    "new_label = train.target[:1]\n",
    "new_label = torch.tensor(new_label).to(float).reshape(new_label.shape, 1)\n",
    "\n",
    "print(new_input_ids)\n",
    "print(new_masks)\n",
    "print(new_label)\n",
    "\n",
    "\n",
    "print(new_model(new_input_ids, token_type_ids=None, attention_mask=new_masks, labels=new_label))\n",
    "print(\"\\n\")\n",
    "\n",
    "texts = train.text.values.tolist()\n",
    "\n",
    "other_input_ids = []\n",
    "other_masks = []\n",
    "for text in texts:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text=text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=10,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    other_input_ids.append(encoded_text.get(\"input_ids\"))\n",
    "    other_masks.append(encoded_text.get(\"attention_mask\"))\n",
    "\n",
    "other_input_ids = torch.cat(other_input_ids, dim=0)\n",
    "other_masks = torch.cat(other_masks, dim=0)\n",
    "\n",
    "other_label = torch.tensor(train.target.values.tolist())\n",
    "\n",
    "print(other_input_ids)\n",
    "print(other_masks)\n",
    "print(other_label)\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    other_input_ids,\n",
    "    other_masks,\n",
    "    other_label\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    other_input_ids, other_masks, other_label = batch\n",
    "    other_label = other_label.reshape((other_label.shape[0], 1))\n",
    "    other_label = other_label.float()\n",
    "    print(new_model(other_input_ids, token_type_ids=None, attention_mask=other_masks, labels=other_label))\n",
    "\n",
    "\n",
    "\n",
    "# other_label = torch.tensor(train.target[:1])\n",
    "# other_label = other_label.reshape((1, 1))\n",
    "# other_label = other_label.float()\n",
    "# print(other_input_ids)\n",
    "# print(other_masks)\n",
    "# print(other_label)\n",
    "# new_model(other_input_ids, token_type_ids=None, attention_mask=other_masks, labels=other_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "eb30b87a-5dee-4d90-801a-2491eee02b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "(a <= 2).to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "6a4fdefc-b8c4-45f1-b98c-44d3ba994ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\owner\\anaconda3\\lib\\site-packages (4.20.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\~-kenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
